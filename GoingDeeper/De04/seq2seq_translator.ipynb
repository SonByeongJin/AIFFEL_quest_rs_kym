{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f9b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from tqdm import tqdm    # tqdm\n",
    "import random\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "from utils import *\n",
    "from s2s import Encoder, Decoder\n",
    "from loss import loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a544060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfce24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dir = os.getenv('HOME') + \"/aiffel/s2s_translation/\"\n",
    "\n",
    "path_to_train_kor = path_to_dir + \"korean-english-park.train.ko\"\n",
    "path_to_train_eng = path_to_dir + \"korean-english-park.train.en\"\n",
    "\n",
    "path_to_test_kor = path_to_dir + \"korean-english-park.test.ko\"\n",
    "path_to_test_eng = path_to_dir + \"korean-english-park.test.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c2c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = read_data(path_to_train_kor)\n",
    "tgt_train = read_data(path_to_train_eng)\n",
    "\n",
    "src_test = read_data(path_to_test_kor)\n",
    "tgt_test = read_data(path_to_test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6002ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94123\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(src_train))\n",
    "print(len(src_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9d4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 중복 제거\n",
    "unique_pairs = set(zip(src_train, tgt_train))\n",
    "corpus_train = list(unique_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19174458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어: 에드워드 7세가 왈리스 심슨 부인때문에 왕관을 포기했는가 ?\n",
      "영어: <start> why did edward viii give up the throne for wallis simpson ? <end>\n",
      "11152\n"
     ]
    }
   ],
   "source": [
    "enc_corpus_train = []\n",
    "dec_corpus_train = []\n",
    "\n",
    "for pair in corpus_train:\n",
    "    kor, eng = pair[0], pair[1]\n",
    "    kor_preprocessed = preprocess_kor(kor)\n",
    "    eng_preprocessed = preprocess_eng(eng)\n",
    "    \n",
    "    if len(kor_preprocessed) <= 80 and len(eng_preprocessed) <= 80 and len(kor_preprocessed) > 2 and len(eng_preprocessed) > 2:\n",
    "        enc_corpus_train.append(kor_preprocessed)\n",
    "        dec_corpus_train.append(eng_preprocessed)\n",
    "\n",
    "print(\"한국어:\", enc_corpus_train[100])\n",
    "print(\"영어:\", dec_corpus_train[100])\n",
    "\n",
    "print(len(enc_corpus_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3938271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화하기\n",
    "enc_input, enc_tokenizer = tokenize_kor(enc_corpus_train)\n",
    "dec_input, dec_tokenizer = tokenize_eng(dec_corpus_train)\n",
    "\n",
    "# train_test_split을 활용해서 훈련 데이터와 검증 데이터로 분리하기\n",
    "enc_train, enc_val = train_test_split(enc_input, test_size=0.2)\n",
    "dec_train, dec_val = train_test_split(dec_input, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e0a9bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (128, 30, 512)\n",
      "Decoder Output: (128, 1, 12394)\n",
      "Decoder Hidden State: (128, 512)\n",
      "Attention: (128, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units = 512\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44104e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5c49f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    batch_size = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399adffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 70/70 [00:46<00:00,  1.49it/s, Loss 3.5891]\n",
      "Test Epoch  1: 100%|██████████| 18/18 [00:13<00:00,  1.29it/s, Test Loss 3.4296]\n",
      "Epoch  2: 100%|██████████| 70/70 [00:09<00:00,  7.13it/s, Loss 3.3491]\n",
      "Test Epoch  2: 100%|██████████| 18/18 [00:00<00:00, 20.38it/s, Test Loss 3.4620]\n",
      "Epoch  3: 100%|██████████| 70/70 [00:09<00:00,  7.08it/s, Loss 3.3529]\n",
      "Test Epoch  3: 100%|██████████| 18/18 [00:00<00:00, 20.19it/s, Test Loss 3.4736]\n",
      "Epoch  4: 100%|██████████| 70/70 [00:09<00:00,  7.07it/s, Loss 3.3465]\n",
      "Test Epoch  4: 100%|██████████| 18/18 [00:00<00:00, 19.86it/s, Test Loss 3.4833]\n",
      "Epoch  5: 100%|██████████| 70/70 [00:10<00:00,  6.99it/s, Loss 3.3514]\n",
      "Test Epoch  5: 100%|██████████| 18/18 [00:00<00:00, 19.99it/s, Test Loss 3.4941]\n",
      "Epoch  6: 100%|██████████| 70/70 [00:10<00:00,  6.91it/s, Loss 3.3515]\n",
      "Test Epoch  6: 100%|██████████| 18/18 [00:00<00:00, 19.62it/s, Test Loss 3.5012]\n",
      "Epoch  7: 100%|██████████| 70/70 [00:10<00:00,  6.84it/s, Loss 3.3541]\n",
      "Test Epoch  7: 100%|██████████| 18/18 [00:00<00:00, 19.53it/s, Test Loss 3.5126]\n",
      "Epoch  8: 100%|██████████| 70/70 [00:10<00:00,  6.76it/s, Loss 3.3512]\n",
      "Test Epoch  8: 100%|██████████| 18/18 [00:00<00:00, 19.37it/s, Test Loss 3.5102]\n",
      "Epoch  9: 100%|██████████| 70/70 [00:10<00:00,  6.70it/s, Loss 3.3517]\n",
      "Test Epoch  9: 100%|██████████| 18/18 [00:00<00:00, 18.93it/s, Test Loss 3.5184]\n",
      "Epoch 10: 100%|██████████| 70/70 [00:10<00:00,  6.64it/s, Loss 3.3509]\n",
      "Test Epoch 10: 100%|██████████| 18/18 [00:00<00:00, 18.94it/s, Test Loss 3.5265]\n"
     ]
    }
   ],
   "source": [
    "# Define eval_step\n",
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "    \n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "    \n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "# Training Process with validation\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_val.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    encoder,\n",
    "                                    decoder,\n",
    "                                    dec_tokenizer)\n",
    "    \n",
    "        test_loss += test_batch_loss\n",
    "\n",
    "        t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c08ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_kor(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        # Ensure to take the argmax for batch_size=1 and handle it correctly for larger batches\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0], axis=-1).numpy()  # Shape: (batch_size,)\n",
    "        predicted_id = predicted_id[0]\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    \n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49bfebbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_214/3912588258.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 각 문장에 대해 번역 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtranslated_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 번역 결과 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_214/569959081.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence, encoder, decoder)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpredicted_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdec_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdec_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'<end>'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# 번역할 문장 리스트\n",
    "input_sentences = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "# 각 문장에 대해 번역 실행\n",
    "for sentence in input_sentences:\n",
    "    translated_sentence, processed_input, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    # 번역 결과 출력\n",
    "    print(f\"input: {processed_input}\")\n",
    "    print(f\"output: {translated_sentence}\")\n",
    "\n",
    "    # Attention Map 시각화\n",
    "    plot_attention(attention, processed_input, translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97fabe0",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    " * utils, loss, s2s 모듈을 만들어 코드를 간략 하게 하였음.\n",
    "\n",
    " * 최초 아래와 같이 실행 된 후 계속 번역에 실패 하고 있음.\n",
    "\n",
    "![f_f](./error_the_us.png)\n",
    "\n",
    " * 계속 원인을 추적 하고 있음.\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1a636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
